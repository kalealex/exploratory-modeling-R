---
title: "Stimulus generation"
author: "Alex Kale"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(magrittr)
library(forcats)
library(ggplot2)
library(ggdist)
library(gganimate)
library(gamlss)
library(jsonlite)
select <- dplyr::select
```

## Overview

In this document, we set out to create standalone stimulus datasets with a known data generating process for each trial of our experiment. We generate fake datasets for two trial types, in one trial type using a real dataset to seed the relationship between season and temperature in order to get realistic looking data distributions. The other trial type is completely synthetic. For each set of predictor variables selected/generated, we then create a fake data generating process (DGP) based on the way that parameterized regression models make predictions. We manipulate the parameters of this DGP to control the difficulty of the task. We use predictions from these DGPs as the fake outcome variables for each stimulus dataset. This procedure enables us to create realistic looking datasets with known data generating processes to use in our experiment.

### Synthetic relationships and task

Creating the fake data generating process (DGP) entails inventing synthetic relationships in the data to ask participants to assess in our experiment. For example, we will ask participants questions like, "How many tokens out of 100 would you bet that there is an influence of X on Y?" providing instructions like, "Betting 0 indicates certainty that there is no relationship. Betting 100 indicates certainty that there is a relationship. Betting 50 indicates maximum uncertainty." To increase statistical power in a small number of trials, we will also elicit uncertainty in bets by asking questions like, "If you had to make this bet 100 times, how many times would you bet between `initial_bet +- interval`?" 

I call the specific relationships we ask about "trial types". Specifically, we create the following _trial types_:

 * Main effect of `ad_campaign` on `sales` 
 * Direct effect of `temp` on `sales` with `season` as a confounding variable

Across these trial types, we match the datasets we generate in terms how difficult they are to detect/judge. We will use [causal support](https://arxiv.org/abs/2107.13485) as a proxy for task difficulty.

These trial types will be crossed with "model check conditions". These _model check conditions_ are different ways of presenting the synthetic relationships in question:

 * Data only, no model checks showing predictions alongside data
 * Theoretically ideal model checks showing predictions from a well-specified model, _not including_ the target relationship `ad_campaign` or `temp`, alongside data
 * Potentially misleading model checks showing predictions from a mis-specified model, ignoring a known/synthetic/fixed nuisance effect in the DGP (see below), alongside data
 * Alternative model checks showing predictions from a well-specified model, _including_ the target relationship `ad_campaign` or `temp`, alongside data
 
In the case where the model check is mis-specified, it will be missing a _variance effect_ of `ad_campaign` on `sales` in the `ad_campaign` trial type, whereas the mis-specified models will not include a _confounding effect_ of `season` on `sales` in the `temp` trial type. In both cases, mis-specifying the reference model we show to participants could be misleading. This condition represents a possible failure mode of model checks, where earlier misinterpretations accrue through the reference model and interfere with the framing of later judgments of DGP.


## Load seed dataset

This is data on the area of land burned in forest fires in the northeast region of Portugal.

```{r}
df.fires <- read_csv("../datasets/forestfires.csv") %>%
  mutate(
    month = fct_relevel(month, "jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"),
    day = fct_relevel(day, "mon","tue","wed","thu","fri","sat","sun")
  ) %>%
  filter(area != 0.0) %>%
  mutate(
    # area = if_else(area ==0.0,
    #                0.01,
    #                area),
    log_area = log(area)
  ) 

head(df.fires)
```
We will use this dataset to seed the relationship between `month` (aggregated into `season`) and `temp` for our `temp` trial type.


## Create a function to calculate causal support for a given dataset and set of models

Causal support is a Bayesian cognitive model that calculates the posterior probability of one set of explanations/models vs another set of explanations/models for a given dataset. For example, if we want to calculate how much a rational Bayesian agent would believe that a particular advertisement influences sales, we would calculate causal support `cs` based on the log likelihood `ll` of the data `d` under two different models with `sales ~ ad` and without `sales ~ 1` the effect of advertisement:

```
cs =                                     # causal support is a Bayesian update in log odds units:
  [ll(d|sales ~ ad) - ll(d|sales ~ 1)] + # log likelihood ratio
  [log(0.5) - log(0.5)]                  # log ratio of priors
```

The function below provides a flexible way of calculating causal support for a given dataset, an outcome variable, one or two predictors, a target regression term to evaluate causal support for, and an optional submodel for variance effects.

```{r}
causal_support <- function(d, outcome, predictors, target_term, sigma_submodel = "~1") {
  # enumerate mu specs
  mu_specs <- list(paste(outcome, "~", "1", sep = ""), paste(outcome, "~", predictors[[1]], sep = ""))
  if (length(predictors) > 1) {
    mu_specs <- mu_specs %>% append(paste(outcome, "~", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[2]], "+", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[2]], "+", predictors[[1]], ":", predictors[[2]], sep = ""))
  }
  
  # function to calculate log likelihood of data for a given model (assuming Gaussian distribution)
  loglik <- function(mu_spec, sigma_spec, df) {
    # fit model
    mu_spec <- as.formula(mu_spec)
    sigma_spec <- as.formula(sigma_spec)
    model <- eval(bquote(gamlss(.(mu_spec), sigma.fo = .(sigma_spec), data = df)))
    
    # use point estimates of mu and sigma for each observed data point to assign likelihood
    df$mu <- predict(model, type = "response")
    df$logsigma <- predict(model, what = "sigma")
    output <- df %>%
      mutate(
        sigma = exp(logsigma)
      ) %>%
      rowwise() %>%
      mutate(
        elpd_per_obs = dnorm(!!sym(outcome), mu, sigma, log = TRUE)
      ) %>% 
      ungroup() %>%
      summarise(
        ll = sum(elpd_per_obs)
      ) %>% 
      as.double()
  }
  
  # apply log likelihood function for each model
  models <- map(mu_specs, ~loglik(.x, sigma_submodel, d)) %>%
    set_names(mu_specs) %>%
    map_dfr(broom::tidy, .id = ".x") %>%
    rename(
      formula = .x,
      ll = x
    )
  
  # determine regex for target term
  if (grepl("[:]", target_term)) {
    # interaction
    target_split <- strsplit(target_term, ":")[[1]]
    target_regex <- paste(target_split[1], "[:*]", target_split[2], sep = "")
  } else {
    # main effect
    target_regex <- paste("[~+]", target_term, "(+|$)", sep = "")
  }
    
  # calculate causal support:
  # prior probability of each model (assume uniform prior)
  n_models <- nrow(models)
  n_target_models <- models %>% 
    filter(grepl(target_regex, formula)) %>%
    nrow()
  prior <- 1.0 / n_models
  # log likelihood of numerator models, containing the target term
  numerator <- models %>% 
    filter(grepl(target_regex, formula)) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_numerator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # log likelihood of denominator models, NOT containing the target term
  denominator <- models %>% 
    filter(!grepl(target_regex, formula)) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_denominator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # calculate causal support (Bayesian update in log odds units)
  cs <- (numerator - denominator) + (log(n_target_models * prior) - log((n_models - n_target_models) * prior))
}
```

```{r}
causal_support_ad <- function(d) {
  # function to calculate log likelihood of data for a given model (assuming Gaussian distribution)
  loglik <- function(target_effect = FALSE, df) {
    # summary statistics for prior setting
    sim_n <- 10000
    deg_free <- length(df$sales) - 1
    sample_min <- min(df$sales) 
    sample_max <- max(df$sales)
    sample_var <- var(df$sales)
    
    # sample parameters for model
    mu_intercept <- runif(sim_n, sample_min, sample_max)
    if (target_effect) {
      # assume mean diff
      mu_diff <- runif(sim_n, -(sample_max - sample_min), sample_max - sample_min)
    } else {
      # assume null mean diff
      mu_diff <- rep(0, sim_n)
    }
    # scaled inv chi squared dist is conjugate prior for sigma of normal distribution
    sigma_intercept <- sqrt(deg_free * sample_var / rchisq(sim_n, deg_free, 0)) 
    # variance effects are multiplicative and can be larger for datasets with larger overall variance
    logsigma_diff <- rnorm(sim_n, 0, log(sqrt(sample_var)))
    
    # run parameter values through a regression model to generate expected mu and sigma for each observation
    output <- df %>%
      mutate(
        draw = list(seq(from = 1, to = sim_n, by = 1)),
        mu = case_when(
          ad_campaign == "informative" ~ list(mu_intercept),
          ad_campaign == "humorous"    ~ list(mu_intercept + mu_diff)
        ),
        sigma = case_when(
          ad_campaign == "informative" ~ list(sigma_intercept),
          ad_campaign == "humorous"    ~ list(exp(log(sigma_intercept) + logsigma_diff))
        )
      ) %>%
      unnest(cols = c("draw", "mu", "sigma"))
    
    # use sampling dist of expected mu and sigma for each observed data point to assign 
    # likelihood of the dataset under each simulation run
    output %>%
      rowwise() %>%
      mutate(
        elpd_per_obs_per_run = dnorm(sales, mu, sigma, log = TRUE)
      ) %>% 
      group_by(draw) %>%
      summarise(
        ll_per_run = sum(elpd_per_obs_per_run)
      ) %>%
      # Monte Carlo integration over simulation runs/draws to calculate avg ll under this model
      ungroup() %>%
      summarise(
        max_ll = max(ll_per_run), # normalize by max ll to make probabilities comparable across Monte Carlo simulations
        ll = max_ll + log(sum(exp(ll_per_run - max_ll))) - log(sim_n)
      ) %>% 
      select(ll) %>%
      as.double() # returns log likelihood after dropping other columns
  }
  
  # apply log likelihood function for each alternative causal explanation
  explanations <- list(TRUE, FALSE)
  models <- map(explanations, ~loglik(as.logical(.x), d)) %>%
    set_names(explanations) %>%
    map_dfr(broom::tidy, .id = ".x") %>%
    mutate(
      .x = as.logical(.x)
    ) %>%
    rename(
      target_claim = .x,
      ll = x
    )
    
  # calculate causal support:
  # prior probability of each model (assume uniform prior)
  n_models <- nrow(models)
  n_target_models <- models %>% filter(target_claim) %>% nrow()
  prior <- 1 / n_models
  # log likelihood of numerator models, where the target claim is assumed true
  numerator <- models %>% 
    filter(target_claim) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_numerator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # log likelihood of denominator models, where the target claim is assumed false
  denominator <- models %>% 
    filter(!target_claim) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_denominator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # calculate causal support (Bayesian update in log odds units)
  cs <- (numerator - denominator) + (log(n_target_models * prior) - log((n_models - n_target_models) * prior))
}
```

Now, we are ready to label the difficulty of judging the synthetic relationships we will ask participants about in the study. The causal support numbers returned by the function above are the _posterior log odds of the target relationship_ given a family of regression models based on the outcome and set of predictors provided, assuming a uniform prior across candidate models.


## Create a modelcheck function to add model predictions to an input dataframe

In this study, we are investigating the utility of showing model predictions alongside real data for helping analysts reason about data generating process. This function adds predictions from a specified linear model to an input dataframe, enabling us to add model predictions for some stimuli. This function comes from the `modelcheck` R package, a custom implementation of model checks created for this project to run models on an R server and push their predictions to the browser.

```{r}
modelcheck <- function(mu_spec, sigma_spec = "~1", data) {
  # settings
  n_draws <- 5

  # get outcome variable and model names
  outcome_name <- sym(sub("\\~.*", "", gsub(" ", "", mu_spec, fixed = TRUE)))
  model_name <- sym(paste("normal", mu_spec, sigma_spec, sep = "| "))
  
  # fit model
  mu_spec <- as.formula(mu_spec)
  sigma_spec <- as.formula(sigma_spec)
  model <- eval(bquote(gamlss(.(mu_spec), sigma.fo = .(sigma_spec), data = data)))
  
  # get summary statistics describing model predictions
  pred.mu <- predict(model, se.fit = TRUE, type = "response")
  pred.sigma <- predict(model, what = "sigma", se.fit = TRUE)
  output <- data %>%
    mutate(
      mu.expectation = pred.mu$fit,                       # add fitted mu and its standard error to dataframe
      mu.se = pred.mu$se.fit,
      logsigma.expectation = pred.sigma$fit,              # add fitted logsigma and its standard error to dataframe 
      logsigma.se = pred.sigma$se.fit,
      df = df.residual(model)                             # get degrees of freedom
    )
  
  # propagate uncertainty in fit to generate an ensemble of model predictions (mimic a posterior predictive distribution)
  output <- output %>%
    mutate(
      draw = list(1:n_draws),                             # generate list of draw numbers
      t1 = map(df, ~rt(n_draws, .)),                      # simulate draws from t distribution to transform into means
      t2 = map(df, ~rt(n_draws, .))                       # simulate draws from t distribution to transform into log sigma
    ) %>%
    unnest(cols = c("draw", "t1", "t2")) %>%
    mutate(
      mu = t1 * mu.se + mu.expectation,                   # scale and shift t to get a sampling distribution of means
      logsigma = t2 * logsigma.se + logsigma.expectation, # scale and shift t to get a sampling distribution of log sigma
      sigma = exp(logsigma)                               # backtransform to sampling distribution of sigma parameter
    ) %>%
    rowwise() %>%
    mutate(
      # compute predictive distribution
      prediction = rnorm(1, mu, sigma)
    ) %>%
    rename(
      data = !!outcome_name,
      !!model_name := prediction
    ) %>%
    pivot_longer(
      cols = c("data", model_name),
      names_to = "modelcheck_group",
      values_to = as.character(outcome_name)
    ) %>%
    dplyr::select(-one_of("mu.expectation", "mu.se", "logsigma.expectation", "logsigma.se", "df", "t1", "t2", "mu", "logsigma", "sigma"))

  return(output)
}
```

Now we can add model predictions to our synthetic datasets.


## Create synthetic datasets

Here, we pass selected variables through a process that mimics a regression model with known parameters. It takes predictor variables as an input and uses some distributional assumptions and hardcoded effect sizes to generate realistic-looking fake datasets for us to use in our experiment. We will use a similar approach to create a standalone DGP for each stimulus.

We'll start by defining some hard coded effect sizes that we will use for stimulus generation. These are extrapolated from Cohen's large and small effect sizes to generate three approximately matched levels of the strength of relationship (note: we rely on causal support for formal matching, so sampling at these effect sizes merely serve to get us in the ballpark of the desired levels of task difficulty). We have separate ways of encoding effect size for categorical effects (i.e., Cohen's d) vs continuous effects (Peason's r).

```{r}
# effect sizes as Cohen's d
cohens.d <- c(0.05, 0.2, 0.8)

# effect sizes as correlation coefficients
pearsons.r <- c(0.02, 0.1, 0.5)
```

We'll also need to control the variance structure of our fake datasets. We do this by setting up effects on variance and residual variance relative to an arbitrarily chosen outcome standard deviation for each standalone DGP.

```{r}
# variance effects as a proportion of the outcome sd (sigma submodel)
modest.var <- 0.015
vanishing.var <- 0.0015

# R^2, to set the residual standard deviation as a proportion of the outcome sd
# res.sd = outcome.sd * (1 - r.squared)
# these models should explain most of the variance in the data since we are coming up with the "true" DGP
r.squared <- 0.9
```

Last, we'll set a sample size, so that we can control the amount of evidence we are showing for these different relationships. We control sample size rather than manipulating it because prior work has shown people are insensitive to sample size for this kind of judgment, and we have no reason to believe that model check would impact this.

```{r}
sample.size <- 150
```

Now we're ready to create some fake datasets.

### Ad campaigns for studying main effects

We'll set up data about fake advertising campaigns to test how well people can judge simple main effects. In each of these ad campaign DGPs, the outcome variable will be `sales`, and the single predictor with two levels will be `ad_campaign`. These trials represent the simplest possible relationships that could be evaluated

We will sample from two advertising campaigns, using a binomial distribution to create two groups of approximately equal size. The predictor `ad_campaign` is completely synthetic.

```{r}
# select variables
df.ad <- tibble(
    p = list(0.5)
  ) %>%
  mutate(
    ad_campaign = map(p, ~rbinom(100*sample.size, 1, .))
  ) %>%
  unnest(cols = c(p, ad_campaign)) %>%
  mutate(
    ad_campaign = case_when(
      ad_campaign == 0 ~ 'informative',
      ad_campaign == 1 ~ 'humorous'
    )
  ) %>%
  select(ad_campaign)

head(df.ad)
```

For each level of effect size that we want to generate fake datasets for, we'll follow the following steps:

 1. *Resample from the seed dataset.* This will entail selecting 150 observations to use as predictors in each stimulus dataset. For the `ad_campaign` trial type, we'll resample from our binomial process. For the `temperature` trial type, we'll resample from the forestfirest dataset we loaded earlier.
 2. *Calculate location and scale parameters `mu` and `sigma` for each observation.* This entails defining fake parameters to set up a data generating process (DGP) that resembles a regression model. Similar to making predictions from a regression models, we will use these fake parameters to calculate conditional estimates of the location `mu` and scale `sigma` of outcomes for each observation.
 3. *Use location and scale estimates to generate fake outcomes.* For each observation in the dataset, we have conditional location `mu` and scale `sigma` estimates from the previous step. Now we pass these parameters through a Gaussian random number generator to compute synthetic outcomes `sales` associated with each observation we sampled in step 1. We will simulate 50 synthetic outcomes `sales` for each observation, generating a distribution of possible outcomes (which we will choose from later, see step 5). 
 4. *Assign causal support values to each of these datasets.* For each set of possible simulated outcomes `sales` we generated in step 3, we run an algorithm to compute causal support. Causal support rates the posterior log odds that a target claim is true about a given dataset, or basically, how much participants in the study should believe that "Ad campaign has an impact on sales" on the `ad_campaign` trials or that "Temperature has an impact on sales after accounting for the impact of season on sales." on the `temp` trials. 
 5. *Filter down to a handful of synthetic datasets with matching levels of causal support.* Usuing causal support as a measure of the difficulty of assessing the target claim on each trial, we take all of the fake datasets we generated and filter them based on proximity to the following values of `logistic(causal_support)`, {0.2, 0.4, 0.6, 0.8}. The idea is to generate a set of stimuli datasets where the target claim is easier to reject (0.2) or accept (0.8) and more difficult to reject (0.4) or accept (0.6). These represent the desired levels of task difficulty.

To start we must choose an arbitrary standard deviation for our outcome variable `sales` as well as arbirary intercepts for the `mu` and `sigma` submodels. The code block below runs steps 1-4.

```{r echo = TRUE, warning=FALSE, results = 'hide'}
# settings arbitrarily chosen to place outcomes in a believable domain
# getting these right is a matter of guess and check
sd <- 1000 # sd for baseline/reference group
res.sd <- sd * (1 - r.squared)
mu.intercept <- 100500
sigma.intercept <- sqrt(sd^2 - res.sd^2) # assuming independence of learned sigma and residual sd

set.seed(42)
n_sims <- 50
datasets.ad <- tibble()

for (i in 1:length(cohens.d)) {
  # resample
  dataset <- df.ad %>% sample_n(sample.size)
  
  # location and scale parameters for `sales ~ ad_campaign`
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      mu = mu.intercept + 
        case_when(                    # ad_campaign effect on mu
          ad_campaign == "humorous" ~ cohens.d[i] * sd, 
          TRUE                      ~ 0.0
        ),
      log_sigma = log(sigma.intercept) +
        case_when(                    # ad_campaign effect on log_sigma
          ad_campaign == "humorous" ~ modest.var * log(sd), 
          TRUE                      ~ 0.0
        ),
      sigma = exp(log_sigma)
    )
  
  # sample 50 fake datasets
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      draw = list(seq(from = 1, to = n_sims, by = 1)),
      sales = list(rnorm(n_sims, mu, sigma) + rnorm(n_sims, 0, res.sd))
    ) %>%
    unnest(cols = c("draw", "sales")) %>%
    select(ad_campaign, draw, sales)
  
  # calculate causal support for each simulated dataset
  results <- tibble()
  for (j in 1:n_sims) {
    # get one dataset to use
    use_data <- dataset %>%
      filter(draw == j) %>%
      select(-draw)
    
    # calc causal support
    cs <- causal_support(
      d = use_data,
      outcome = "sales", 
      predictors = list("ad_campaign"), 
      target_term ="ad_campaign",
      sigma_submodel = "~ad_campaign"
    )
    
    # store results
    result <- tibble(
        use_data, # important that this is not named
        causal_support = cs,
        normative_belief = plogis(cs),
        effect_size = cohens.d[i],
        draw = j
      ) %>%
      group_by(causal_support, normative_belief, effect_size, draw) %>%
      nest()
    results <- bind_rows(results, result)
  }
  
  # append resulting datasets to output
  datasets.ad <- bind_rows(datasets.ad, results)
}

head(datasets.ad)
```

Causal support for the relationship of interest represents a log odds transform of the normative probability that the target relationship is part of the data generating process. In this trial type, this is how a rational Bayesian agent would rate the probability of the following claim:

_"Ad campaign has an impact on sales."_

Now, we'll pick 4 fake datasets per level of task difficulty (one dataset to show for each modelcheck condition), resulting in 12 datasets total for this trial type.

```{r echo = TRUE, warning=FALSE, results = 'hide'}
target_normative_response <- c(0.2, 0.4, 0.6, 0.8)
save_datasets <- list()

for (i in 1:length(target_normative_response)) {
  # select generated datasets closest to target causal support
  selected_datasets <- datasets.ad %>%
    mutate(
      diff_from_target = abs(target_normative_response[i] - normative_belief)
    ) %>%
    slice_min(diff_from_target, n = 4) %>%
    select(causal_support, normative_belief, data)
  
  # add correctly specified model check to second selected dataset
  selected_datasets[2, "data"] <- modelcheck(
      "sales ~ 1",
      "~ad_campaign",
      unnest(selected_datasets[2, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add mis-specified model check to third selected dataset
  selected_datasets[3, "data"] <- modelcheck(
      "sales ~ 1",
      "~1",                        # ignore real variance effect
      unnest(selected_datasets[3, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add alternative model check to fourth selected dataset
  selected_datasets[4, "data"] <- modelcheck(
      "sales ~ ad_campaign",      # model target relationship
      "~ad_campaign",       
      unnest(selected_datasets[4, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # create the data structure we want
  save_datasets[[paste("dataonly", target_normative_response[i]*100, "main",  sep = "_")]] <- selected_datasets[1,]
  save_datasets[[paste("modelcheck", target_normative_response[i]*100, "main", sep = "_")]] <- selected_datasets[2,]
  save_datasets[[paste("misspec", target_normative_response[i]*100, "main", sep = "_")]] <- selected_datasets[3,] 
  save_datasets[[paste("altcheck", target_normative_response[i]*100, "main", sep = "_")]] <- selected_datasets[4,] 
}
```


### Interaction effects with confouding

We'll set up synthetic dataset to test how well people can judge interaction effects. In each of these DGPs, there will be two correlated predictors (`season` and `temp`) representing a confounding "backdoor path" for the direct relationship the participant will be asked to judge. The purpose of these trails is to investigate more complex relationships that the analyst must build a more sophisticated interpretation of. By manipulating the veracity of prior knowledge they are given about the backdoor path (i.e., by mis-specifying the model check so that it doesn't account for confounding on some trials), we can investigate how robust model checks are to reference model mis-specification compared to how robust EDA is to inaccurate prior knowledge. This is important to examine because part of the purpose of model checks is making these provisional data interpretations explicit to support cumulative judgments of DGP.

For this dataset we'll just take the `month` and `temp` predictors directly from the forestfires dataset. We will reduce the cardinality of the month variable by binning months into a new variable called `season`. Using seasons instead of months will make it easier to plot our model checks.

```{r}
# select variables
df.temp <- df.fires %>%
  mutate(
    season = case_when( # reduce cardinality of month for better plots
      month == "feb" ~ "winter", 
      month == "mar" ~ "spring", 
      month == "apr" ~ "spring", 
      month == "may" ~ "spring", 
      month == "jun" ~ "summer", 
      month == "jul" ~ "summer", 
      month == "aug" ~ "summer",
      month == "sep" ~ "fall",
      month == "oct" ~ "fall",
      month == "nov" ~ "fall",
      month == "dec" ~ "winter",
      TRUE           ~ "winter" # case "jan"
    ),
    season = fct_relevel(season, "winter","spring","summer","fall"),
  ) %>%
  select(season, temp)
  # select(month, temp)

head(df.temp)
```

We'll follow a similar procedure as before for dataset generation:

 1. Resample from the seed dataset.
 2. Calculate location and scale parameters `mu` and `sigma` for each observation in the seed dataset using fake parameters we plant in the DGP
 3. Use the resulting location and scale parameters to generate 50 outcomes per dataset in step 1.
 4. Assign causal support values to each of these datasets.
 5. Filter down to a handful of fake datasets where causal support is close to 0.2, 0.4, 0.6, and 0.8, respectively.
 
See the rationale and fleshed out description for each of these steps above.

Again, we must choose an arbitrary standard deviation for our outcome variable `sales` as well as arbirary intercepts for the `mu` and `sigma` submodels. The code block below implements steps 1-4.

```{r echo = TRUE, warning=FALSE, results = 'hide'}
# settings arbitrarily chosen to place outcomes in a believable domain
# getting these right is a matter of guess and check
sd <- 70 # sd for baseline/reference group
res.sd <- sd * (1 - r.squared)
mu.intercept <- 400
sigma.intercept <- sqrt(sd^2 - res.sd^2) # assuming independence of learned sigma and residual sd

# also need
temp.sd <- sd(df.temp$temp)

set.seed(44)
n_sims <- 50
datasets.temp <- tibble()

for (i in 1:length(pearsons.r)) {
  # resample
  dataset <- df.temp %>% sample_n(sample.size)
  
  # location and scale parameters for `sales ~ season*temp`
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      mu = mu.intercept + 
        case_when(       # season effect on mu
          season == "winter" ~ -cohens.d[2] * sd, 
          season == "summer" ~ cohens.d[2] * sd, 
          TRUE               ~ 0.0 # case "spring", "fall"
        ) + 
        # case_when(       # month effect on mu
        #   month == "feb" ~ -cohens.d[2] * sd, 
        #   month == "mar" ~ 0.0, 
        #   month == "apr" ~ 0.0, 
        #   month == "may" ~ 0.0, 
        #   month == "jun" ~ cohens.d[2] * sd, 
        #   month == "jul" ~ cohens.d[2] * sd, 
        #   month == "aug" ~ cohens.d[2] * sd,
        #   month == "sep" ~ cohens.d[2] * sd,
        #   month == "oct" ~ 0.0,
        #   month == "nov" ~ 0.0,
        #   month == "dec" ~ -cohens.d[2] * sd,
        #   TRUE           ~ -cohens.d[2] * sd # case "jan"
        # ) + 
        temp * pearsons.r[i] * sd / temp.sd, # temp effect on mu
      log_sigma = log(sigma.intercept) +
        log(temp) * vanishing.var * log(sd),      # temp effect on sigma,
      sigma = exp(log_sigma)
    )
  
  # sample 50 fake datasets
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      draw = list(seq(from = 1, to = n_sims, by = 1)),
      sales = list(rnorm(n_sims, mu, sigma) + rnorm(n_sims, 0, res.sd))
    ) %>%
    unnest(cols = c("draw", "sales")) %>%
    select(season, temp, draw, sales)
    # select(month, temp, draw, sales)
  
  # calculate causal support for each simulated dataset
  results <- tibble()
  for (j in 1:n_sims) {
    # get one dataset to use
    use_data <- dataset %>%
      filter(draw == j) %>%
      select(-draw)
    
    # calc causal support
    cs <- causal_support(
      d = use_data,
      outcome = "sales", 
      predictors = list("season", "temp"), 
      # predictors = list("month", "temp"), 
      target_term ="temp",
      sigma_submodel = "~temp"
    )
    
    # store results
    result <- tibble(
        use_data, # important that this is not named
        causal_support = cs,
        normative_belief = plogis(cs),
        effect_size = pearsons.r[i],
        draw = j
      ) %>%
      group_by(causal_support, normative_belief, effect_size, draw) %>%
      nest()
    results <- bind_rows(results, result)
  }
  
  # append resulting datasets to output
  datasets.temp <- bind_rows(datasets.temp, results)
}

head(datasets.temp)
```

In this trial type, causal support provides a benchmark of how a rational Bayesian agent would rate the probability of the following claim:

_"Temperature has an impact on sales (after accounting for the impact of season on sales)."_

Now, we'll pick 4 fake datasets per level of task difficulty (one dataset to show for each modelcheck condition), resulting in 12 datasets total for this trial type.

```{r echo = TRUE, warning=FALSE, results = 'hide'}
for (i in 1:length(target_normative_response)) {
  # select generated datasets closest to target causal support
  selected_datasets <- datasets.temp %>%
    mutate(
      diff_from_target = abs(target_normative_response[i] - normative_belief)
    ) %>%
    slice_min(diff_from_target, n = 4) %>%
    select(causal_support, normative_belief, data)
  
  # add correctly specified model check to second selected dataset
  selected_datasets[2, "data"] <- modelcheck(
      "sales ~ season",
      # "sales ~ month",
      "~temp",
      unnest(selected_datasets[2, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add mis-specified model check to third selected dataset
  selected_datasets[3, "data"] <- modelcheck(
      "sales ~ 1",                    # ignore real confound
      "~temp",                        
      unnest(selected_datasets[3, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add alternative model check to fourth selected dataset
  selected_datasets[4, "data"] <- modelcheck(
      "sales ~ season*temp",          # model target relationship
      # "sales ~ month*temp",          # model target relationship
      "~temp",
      unnest(selected_datasets[4, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # create the data structure we want
  save_datasets[[paste("dataonly", target_normative_response[i]*100, "interaction",  sep = "_")]] <- selected_datasets[1,]
  save_datasets[[paste("modelcheck", target_normative_response[i]*100, "interaction", sep = "_")]] <- selected_datasets[2,]
  save_datasets[[paste("misspec", target_normative_response[i]*100, "interaction", sep = "_")]] <- selected_datasets[3,] 
  save_datasets[[paste("altcheck", target_normative_response[i]*100, "interaction", sep = "_")]] <- selected_datasets[4,] 
}
```

Save datasets. We'll push these synthetic datasets to Firebase and query them by trial type name on the fly in order to run our experiment in a custom web application.

```{r eval=FALSE}
# save datasets as json (ready to upload to firebase)
write(toJSON(save_datasets), "stimuli.json")
```


## Select datasets for dissertation defense

### RQ1: Do well-specified model checks help users assess claims about target relationships in a dataset?

Ad campaign trials.

```{r}
test <- save_datasets[[paste("dataonly", target_normative_response[4]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  ggplot(aes(x = ad_campaign, y = sales)) +
  geom_point(shape = "_", color = "steelblue", size  = 5) +
  theme_minimal()
```

```{r}
test <- save_datasets[[paste("modelcheck", target_normative_response[4]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = ad_campaign, y = sales, color = modelcheck_group, group = modelcheck_group)) +
  geom_point(shape = "_", size  = 5, position = position_dodge(0.2)) +
  theme_minimal()
```

Temperature trials.

```{r}
test <- save_datasets[[paste("dataonly", target_normative_response[4]*100, "interaction",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  ggplot(aes(x = temp, y = sales)) +
  geom_point(color ="steelblue") +
  theme_minimal() +
  facet_grid(. ~ season)
```

```{r}
test <- save_datasets[[paste("modelcheck", target_normative_response[4]*100, "interaction",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = temp, y = sales, color = modelcheck_group)) +
  geom_point() +
  theme_minimal() +
  facet_grid(. ~ season)
# write(toJSON(test), "modelcheck.json")
```

### RQ2: Do mis-specified model checks lead to misinterpretations of DGP?

Missing variance effect.

```{r}
test <- save_datasets[[paste("misspec", target_normative_response[4]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = ad_campaign, y = sales, color = modelcheck_group, group = modelcheck_group)) +
  geom_point(shape = "_", size  = 5, position = position_dodge(0.2)) +
  theme_minimal()
```

Missing confounding effect.

```{r}
test <- save_datasets[[paste("misspec", target_normative_response[4]*100, "interaction",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = temp, y = sales, color = modelcheck_group)) +
  geom_point() +
  theme_minimal() +
  facet_grid(. ~ season)
# write(toJSON(test), "misspec.json")
```

### RQ3: Should the framing of a model check depend on whether the target claim appears to be true? 

Traditional model check, target relationship not in model.

```{r}
test <- save_datasets[[paste("modelcheck", target_normative_response[4]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = ad_campaign, y = sales, color = modelcheck_group, group = modelcheck_group)) +
  geom_point(shape = "_", size  = 5, position = position_dodge(0.2)) +
  theme_minimal()
```

Not including the target relationship in the reference model might not work as well when the target relationship is likely false. This becomes a judgment of similarity between data and model predictions, rather than a judgment of discrepancy.

```{r}
test <- save_datasets[[paste("modelcheck", target_normative_response[1]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = ad_campaign, y = sales, color = modelcheck_group, group = modelcheck_group)) +
  geom_point(shape = "_", size  = 5, position = position_dodge(0.2)) +
  theme_minimal()
```

In these cases where the user would like to reject the target claim, it might help to use an alternative configuration of model checks which shows what it would look like if the target relationship were true.

```{r}
test <- save_datasets[[paste("altcheck", target_normative_response[1]*100, "main",  sep = "_")]]
test <- as.data.frame(test$data)

test %>%
  filter(draw == 1) %>%
  ggplot(aes(x = ad_campaign, y = sales, color = modelcheck_group, group = modelcheck_group)) +
  geom_point(shape = "_", size  = 5, position = position_dodge(0.2)) +
  theme_minimal()
```







