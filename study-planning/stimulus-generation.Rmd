---
title: "Stimulus generation"
author: "Alex Kale"
date: "3/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(magrittr)
library(forcats)
library(ggplot2)
library(ggdist)
library(gganimate)
library(gamlss)
library(jsonlite)
select <- dplyr::select
```

## Overview

In this document, we set out to create standalone datasets with a known data generating process for each trial of our experiment. We load in seed datasets and use them to generate fake data. To create each stimulus, we select predictors from these seed datasets and rename the variables in order to get realistic looking data distributions. For each set of variables selected, we then create a fake data generating process (DGP) based on the way that parameterized regression models make predictions. We manipulate the parameters of this DGP to control the difficulty of the task. We use predictions from these DGPs as the fake outcome variables for each stimulus dataset. This procedure enables us to create realistic looking datasets with known data generating processes to use in our experiment.

### Synthetic relationships and task

Creating the fake data generating process (DGP) entails inventing synthetic relationships in the data to ask participants to assess in our experiment. For example, we will ask participants questions like, "How many tokens out of 100 would you bet that there is an influence of X on Y?" providing instructions like, "Betting 0 indicates certainty that there is no relationship. Betting 100 indicates certainty that there is a relationship. Betting 50 indicates maximum uncertainty." To increase statistical power in a small number of trials, we will also elicit uncertainty in bets by asking questions like, "If you had to make this bet 100 times, how many times would you bet between `initial_bet +- interval`?" 

I call the specific relationships we ask about "information conditions". Specifically, we create the following _information conditions_:

 * Main effect of `ad_campaign` on `sales` 
 * Direct effect of `temp` on `sales` with `month` as a confounding variable

Across these information conditions, we match the datasets we generate in terms how difficult they are to detect/judge. We will use [causal support](https://arxiv.org/abs/2107.13485) as a proxy for task difficulty.

These information conditions will be crossed with "model check conditions". These are different ways of presenting the synthetic relationships in question:

 * Data only, no model checks showing predictions alongside data
 * Model checks showing predictions from a properly specified model alongside data
 * Model checks showing predictions from a mis-specified model
 
In the case where the model check is mis-specified, it will be missing a variance effect in the information condition where the user judges a "Main effect of `ad_campaign` on `sales`," whereas the mis-specified models will not include the effect of the mediating or confounding variables in the other information conditions.


## Load seed datasets

This is data on the area of land burned in forest fires in the northeast region of Portugal.

```{r}
df.fires <- read_csv("../datasets/forestfires.csv") %>%
  mutate(
    month = fct_relevel(month, "jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"),
    day = fct_relevel(day, "mon","tue","wed","thu","fri","sat","sun")
  ) %>%
  filter(area != 0.0) %>%
  mutate(
    # area = if_else(area ==0.0,
    #                0.01,
    #                area),
    log_area = log(area)
  ) 

head(df.fires)
```

This is data on student absences in two schools in Portugal.

```{r}
# load math and language class data separately
df_mat <- read.table("../datasets/student-mat.csv", sep=";", header=TRUE) %>%
  dplyr::select(-one_of(c("G1","G2"))) %>%
  mutate(topic = "math")
df_lan <- read.table("../datasets/student-por.csv", sep=";", header=TRUE) %>%
  dplyr::select(-one_of(c("G1","G2"))) %>%
  mutate(topic = "language")

# join into one dataset for analysis
df.absences <- df_mat %>%
  full_join(df_lan, by = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","guardian","traveltime","studytime","failures","schoolsup","famsup","paid","activities","nursery","higher","internet","romantic","famrel","freetime","goout", "Dalc","Walc","health","absences","G3","topic")) %>%
  mutate(
    address = if_else(address=="U", "urban", "rural"),
    Pstatus = if_else(Pstatus=="A", "separated", "together")
  ) %>%
  filter(absences != 0) %>%
  mutate(
    # absences = if_else(absences == 0,
    #                    0.01,
    #                    as.double(absences)),
    log_absences = log(as.double(absences))
  ) %>%
  rename(
    final_grade = G3
  )

head(df.absences)
```

## Create a function to calculate causal support for a given dataset and set of models

Causal support is a Bayesian cognitive model that calculates the posterior probability of one set of explanations/models vs another set of explanations/models for a given dataset. For example, if we want to calculate how much a rational Bayesian agent would believe that a particular advertisement influences sales, we would calculate causal support `cs` based on the log likelihood `ll` of the data `d` under two different models with `sales ~ ad` and without `sales ~ 1` the effect of advertisement:

```
cs =                                     # causal support is a Bayesian update in log odds units:
  [ll(d|sales ~ ad) - ll(d|sales ~ 1)] + # log likelihood ratio
  [log(0.5) - log(0.5)]                  # log ratio of priors
```

The function below provides a flexible way of calculating causal support for a given dataset, an outcome variable, one or two predictors, a target regression term to evaluate causal support for, and an optional submodel for variance effects.

```{r}
causal_support <- function(d, outcome, predictors, target_term, sigma_submodel = "~1") {
  # enumerate mu specs
  mu_specs <- list(paste(outcome, "~", "1", sep = ""), paste(outcome, "~", predictors[[1]], sep = ""))
  if (length(predictors) > 1) {
    mu_specs <- mu_specs %>% append(paste(outcome, "~", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[2]], "+", predictors[[1]], ":", predictors[[2]], sep = "")) %>%
      append(paste(outcome, "~", predictors[[1]], "+", predictors[[2]], "+", predictors[[1]], ":", predictors[[2]], sep = ""))
  }
  
  # function to calculate log likelihood of data for a given model (assuming Gaussian distribution)
  loglik <- function(mu_spec, sigma_spec, df) {
    # fit model
    mu_spec <- as.formula(mu_spec)
    sigma_spec <- as.formula(sigma_spec)
    model <- eval(bquote(gamlss(.(mu_spec), sigma.fo = .(sigma_spec), data = df)))
    
    # use point estimates of mu and sigma for each observed data point to assign likelihood
    df$mu <- predict(model, type = "response")
    df$logsigma <- predict(model, what = "sigma")
    output <- df %>%
      mutate(
        sigma = exp(logsigma)
      ) %>%
      rowwise() %>%
      mutate(
        elpd_per_obs = dnorm(!!sym(outcome), mu, sigma, log = TRUE)
      ) %>% 
      ungroup() %>%
      summarise(
        ll = sum(elpd_per_obs)
      ) %>% 
      as.double()
  }
  
  # apply log likelihood function for each model
  models <- map(mu_specs, ~loglik(.x, sigma_submodel, d)) %>%
    set_names(mu_specs) %>%
    map_dfr(broom::tidy, .id = ".x") %>%
    rename(
      formula = .x,
      ll = x
    )
  
  # determine regex for target term
  if (grepl("[:]", target_term)) {
    # interaction
    target_split <- strsplit(target_term, ":")[[1]]
    target_regex <- paste(target_split[1], "[:*]", target_split[2], sep = "")
  } else {
    # main effect
    target_regex <- paste("[~+]", target_term, "(+|$)", sep = "")
  }
    
  # calculate causal support:
  # prior probability of each model (assume uniform prior)
  n_models <- nrow(models)
  n_target_models <- models %>% 
    filter(grepl(target_regex, formula)) %>%
    nrow()
  prior <- 1.0 / n_models
  # log likelihood of numerator models, containing the target term
  numerator <- models %>% 
    filter(grepl(target_regex, formula)) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_numerator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # log likelihood of denominator models, NOT containing the target term
  denominator <- models %>% 
    filter(!grepl(target_regex, formula)) %>%
    mutate(
      llmax = max(ll)
    ) %>%
    ungroup() %>%
    summarise(
      ll_denominator = unique(llmax) + log(sum(exp(ll - unique(llmax))))
    ) %>%
    as.double()
  # calculate causal support (Bayesian update in log odds units)
  cs <- (numerator - denominator) + (log(n_target_models * prior) - log((n_models - n_target_models) * prior))
}
```

Now, we are ready to label the difficulty of judging the synthetic relationships we will ask participants about in the study. The causal support numbers returned by the function above are the _posterior log odds of the target relationship_ given a family of regression models based on the outcome and set of predictors provided, assuming a uniform prior across candidate models.


## Create a modelcheck function to add model predictions to an input dataframe

In this study, we are investigating the utility of showing model predictions alongside real data for helping analysts reason about data generating process. This function adds predictions from a specified linear model to an input dataframe, enabling us to add model predictions for some stimuli.

```{r}
modelcheck <- function(mu_spec, sigma_spec = "~1", data) {
  # settings
  n_draws <- 5

  # get outcome variable and model names
  outcome_name <- sym(sub("\\~.*", "", gsub(" ", "", mu_spec, fixed = TRUE)))
  model_name <- sym(paste("normal", mu_spec, sigma_spec, sep = "| "))
  
  # fit model
  mu_spec <- as.formula(mu_spec)
  sigma_spec <- as.formula(sigma_spec)
  model <- eval(bquote(gamlss(.(mu_spec), sigma.fo = .(sigma_spec), data = data)))
  
  # get summary statistics describing model predictions
  pred.mu <- predict(model, se.fit = TRUE, type = "response")
  pred.sigma <- predict(model, what = "sigma", se.fit = TRUE)
  output <- data %>%
    mutate(
      mu.expectation = pred.mu$fit,                       # add fitted mu and its standard error to dataframe
      mu.se = pred.mu$se.fit,
      logsigma.expectation = pred.sigma$fit,              # add fitted logsigma and its standard error to dataframe 
      logsigma.se = pred.sigma$se.fit,
      df = df.residual(model)                             # get degrees of freedom
    )
  
  # propagate uncertainty in fit to generate an ensemble of model predictions (mimic a posterior predictive distribution)
  output <- output %>%
    mutate(
      draw = list(1:n_draws),                             # generate list of draw numbers
      t1 = map(df, ~rt(n_draws, .)),                      # simulate draws from t distribution to transform into means
      t2 = map(df, ~rt(n_draws, .))                       # simulate draws from t distribution to transform into log sigma
    ) %>%
    unnest(cols = c("draw", "t1", "t2")) %>%
    mutate(
      mu = t1 * mu.se + mu.expectation,                   # scale and shift t to get a sampling distribution of means
      logsigma = t2 * logsigma.se + logsigma.expectation, # scale and shift t to get a sampling distribution of log sigma
      sigma = exp(logsigma)                               # backtransform to sampling distribution of sigma parameter
    ) %>%
    rowwise() %>%
    mutate(
      # compute predictive distribution
      prediction = rnorm(1, mu, sigma)
    ) %>%
    rename(
      data = !!outcome_name,
      !!model_name := prediction
    ) %>%
    pivot_longer(
      cols = c("data", model_name),
      names_to = "modelcheck_group",
      values_to = as.character(outcome_name)
    ) %>%
    dplyr::select(-one_of("mu.expectation", "mu.se", "logsigma.expectation", "logsigma.se", "df", "t1", "t2", "mu", "logsigma", "sigma"))

  return(output)
}
```

Now we can add model predictions to our synthetic datasets.


## Create synthetic datasets

Here, we pass selected variables through a process that mimics a regression model with known parameters. It takes part of a real dataset with renamed variables as an input and uses that dataset in combination with some distributional assumptions and hardcoded effect sizes to generate a realistic-looking fake dataset for us to use in our experiment. We will use a similar approach to create a standalone DGP for each stimulus.

We'll start by defining some hard coded effect sizes that we will use for stimulus generation. These are extrapolated from Cohen's large and small effect sizes to generate three approximately matched levels of the strength of relationship (note: we rely on causal support for formal matching, so sampling at these effect sizes merely serve to get us in the ballpark of the desired levels of task difficulty). We have separate ways of encoding effect size for categorical effects (i.e., Cohen's d) vs continuous effects (Peason's r).

```{r}
# effect sizes as Cohen's d
cohens.d <- c(0.05, 0.2, 0.8)

# effect sizes as correlation coefficients
pearsons.r <- c(0.02, 0.1, 0.5)
```

We'll also need to control the variance structure of our fake datasets. We do this by setting up effects on variance and residual variance relative to an arbitrarily chosen outcome standard deviation for each standalone DGP.

```{r}
# variance effects as a proportion of the outcome sd (sigma submodel)
modest.var <- 0.015
vanishing.var <- 0.0015

# R^2, to set the residual standard deviation as a proportion of the outcome sd
# res.sd = outcome.sd * (1 - r.squared)
# these models should explain most of the variance in the data since we are coming up with the "true" DGP
r.squared <- 0.9
```

Last, we'll set a sample size, so that we can control the amount of evidence we are showing for these different relationships. We control sample size rather than manipulating it because prior work has shown people are insensitive to sample size for this kind of judgment, and we have no reason to believe that model check would impact this

```{r}
sample.size <- 150
```

Now we're ready to create some fake datasets.

### Ad campaigns for studying main effects

We'll set up data about fake advertising campaigns to test how well people can judge simple main effects. In each of these ad campaign DGPs, the outcome variable will be `sales`, and the single predictor with two levels will be `ad_campaign`. These trials represent the simplest possible relationships that could be evaluated

To seed our ad campaign stimuli, we'll use the `Fedu` variable from the student absences dataset which originally represents a father's level of education. The fact that these data come from a different domain than `sales ~ ad_campaign` doesn't really concern us because we are just borrowing them to get realistically distributed predictors.

```{r}
df.ad <- df.absences %>% select(Fedu)
```

#### Humorous vs informative ads

For the humorous vs traditional ads we'll select two levels of `Fedu`.

```{r}
# select variables
df.ad <- df.ad %>%
  filter(Fedu %in% c(2, 3)) %>%
  mutate(
    ad_campaign = case_when(
      Fedu == 2 ~ 'informative',
      Fedu == 3 ~ 'humorous'
    )
  ) %>%
  select(ad_campaign)

head(df.ad)
```
```{r}
any(is.na(df.ad$ad_campaign))
```

For each level of effect size we want to generate fake datasets for, we'll follow the following steps:

 1. Resample from the seed dataset.
 2. Calculate location and scale parameters `mu` and `sigma` for each observation in the seed dataset using fake parameters we plant in the DGP
 3. Use the resulting location and scale parameters to generate 50 fake datasets.
 4. Assign causal support values to each of these datasets.

We subsequently will filter down to a handful of fake datasets where causal support is close to 0.55, 0.75, and 0.95, respectively, representing three desired levels of task difficulty.

To start we must choose an arbitrary standard deviation for our outcome variable `sales` as well as arbirary intercepts for the `mu` and `sigma` submodels.

```{r}
# settings arbitrarily chosen to place outcomes in a believable domain
# getting these right is a matter of guess and check
sd <- 1000 # sd for baseline/reference group
res.sd <- sd * (1 - r.squared)
mu.intercept <- 100500
sigma.intercept <- sqrt(sd^2 - res.sd^2) # assuming independence of learned sigma and residual sd

set.seed(42)
n_sims <- 50
datasets.ad <- tibble()

for (i in 1:length(cohens.d)) {
  # resample
  dataset <- df.ad %>% sample_n(sample.size)
  
  # location and scale parameters for `sales ~ ad_campaign`
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      mu = mu.intercept + 
        case_when(                    # ad_campaign effect on mu
          ad_campaign == "humorous" ~ cohens.d[i] * sd, 
          TRUE                      ~ 0.0
        ),
      log_sigma = log(sigma.intercept) +
        case_when(                    # ad_campaign effect on log_sigma
          ad_campaign == "humorous" ~ modest.var * log(sd), 
          TRUE                      ~ 0.0
        ),
      sigma = exp(log_sigma)
    )
  
  # sample 50 fake datasets
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      draw = list(seq(from = 1, to = n_sims, by = 1)),
      sales = list(rnorm(n_sims, mu, sigma) + rnorm(n_sims, 0, res.sd))
    ) %>%
    unnest(cols = c("draw", "sales")) %>%
    select(ad_campaign, draw, sales)
  
  # calculate causal support for each simulated dataset
  results <- tibble()
  for (j in 1:n_sims) {
    # get one dataset to use
    use_data <- dataset %>%
      filter(draw == j) %>%
      select(-draw)
    
    # calc causal support
    cs <- causal_support(
      d = use_data,
      outcome = "sales", 
      predictors = list("ad_campaign"), 
      target_term ="ad_campaign",
      sigma_submodel = "~ad_campaign"
    )
    
    # store results
    result <- tibble(
        use_data, # important that this is not named
        causal_support = cs,
        normative_belief = plogis(cs),
        effect_size = cohens.d[i],
        draw = j
      ) %>%
      group_by(causal_support, normative_belief, effect_size, draw) %>%
      nest()
    results <- bind_rows(results, result)
  }
  
  # append resulting datasets to output
  datasets.ad <- bind_rows(datasets.ad, results)
}

head(datasets.ad)
```

Causal support for the relationship of interest represents a log odds transform of the normative probability that the target relationship is part of the data generating process. In this information condition, this is how a rational Bayesian agent would rate the probability of the following claim:

_"Ad campaign has an impact on sales."_

Now, we'll pick 3 fake datasets per level of task difficulty (one dataset to show for each modelcheck condition), resulting in 9 datasets total for this information condition

```{r}
target_normative_response <- c(0.55, 0.75, 0.95)
save_datasets <- list()

for (i in 1:length(target_normative_response)) {
  # select generated datasets closest to target causal support
  selected_datasets <- datasets.ad %>%
    mutate(
      diff_from_target = abs(target_normative_response[i] - normative_belief)
    ) %>%
    slice_min(diff_from_target, n = 3) %>%
    select(causal_support, normative_belief, data)
  
  # add correctly specified model check to second selected dataset
  selected_datasets[2, "data"] <- modelcheck(
      "sales ~ 1",
      "~ad_campaign",
      unnest(selected_datasets[2, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add mis-specified model check to third selected dataset
  selected_datasets[3, "data"] <- modelcheck(
      "sales ~ 1",
      "~1",                        # ignore real variance effect
      unnest(selected_datasets[3, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # create the data structure we want
  save_datasets[[paste("dataonly", target_normative_response[i], "main",  sep = "_")]] <- selected_datasets[1,]
  save_datasets[[paste("modelcheck", target_normative_response[i], "main", sep = "_")]] <- selected_datasets[2,]
  save_datasets[[paste("misspec", target_normative_response[i], "main", sep = "_")]] <- selected_datasets[3,] 
}
```


### Interaction effects

We'll set up synthetic dataset to test how well people can judge interaction effects. In each of these DGPs, there will be two correlated predictors (`month` and `temp`) representing a confounding "backdoor path" for the direct relationship the participant will be asked to judge. The purpose of these trails is to investigate more complex relationships that the analyst must build a more sophisticated interpretation of. By manipulating the veracity of prior knowledge they are given about the backdoor path (i.e., by mis-specifying the model check so that it doesn't account for confounding on some trials), we can investigate how robust model checks are to reference model mis-specification compared to how robust EDA is to inaccurate prior knowledge. This is important to examine because part of the purpose of model checks is making these provisional data interpretations explicit to support cumulative judgments of DGP.

#### Effect of temperature on sales after controlling for month (confounding)

For this dataset we'll just take the `month` and `temp` predictors directly from the forestfires dataset. We don't need to rename them.

```{r}
# select variables
df.month.temp <- df.fires %>%
  select(month, temp)

head(df.month.temp)
```

We'll follow a similar procedure as before for dataset generation:

 1. Resample from the seed dataset.
 2. Calculate location and scale parameters `mu` and `sigma` for each observation in the seed dataset using fake parameters we plant in the DGP
 3. Use the resulting location and scale parameters to generate 50 fake datasets.
 4. Assign causal support values to each of these datasets.

We subsequently will filter down to a handful of fake datasets where causal support is close to 0.55, 0.75, and 0.95, respectively, representing three desired levels of task difficulty.

Again, we must choose an arbitrary standard deviation for our outcome variable `sales` as well as arbirary intercepts for the `mu` and `sigma` submodels.

```{r}
# settings arbitrarily chosen to place outcomes in a believable domain
# getting these right is a matter of guess and check
sd <- 70 # sd for baseline/reference group
res.sd <- sd * (1 - r.squared)
mu.intercept <- 400
sigma.intercept <- sqrt(sd^2 - res.sd^2) # assuming independence of learned sigma and residual sd

# also need
temp.sd <- sd(df.month.temp$temp)

set.seed(44)
n_sims <- 50
datasets.month.temp <- tibble()

for (i in 1:length(pearsons.r)) {
  # resample
  dataset <- df.month.temp %>% sample_n(sample.size)
  
  # location and scale parameters for `sales ~ month*temp`
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      mu = mu.intercept + 
        case_when(       # month effect on mu
          month == "feb" ~ -cohens.d[2] * sd, 
          month == "mar" ~ 0.0, 
          month == "apr" ~ 0.0, 
          month == "may" ~ 0.0, 
          month == "jun" ~ cohens.d[2] * sd, 
          month == "jul" ~ cohens.d[2] * sd, 
          month == "aug" ~ cohens.d[2] * sd,
          month == "sep" ~ cohens.d[2] * sd,
          month == "oct" ~ 0.0,
          month == "nov" ~ 0.0,
          month == "dec" ~ -cohens.d[2] * sd,
          TRUE           ~ -cohens.d[2] * sd # case "jan"
        ) + 
        temp * pearsons.r[i] * sd / temp.sd, # temp effect on mu
      log_sigma = log(sigma.intercept) +
        log(temp) * vanishing.var * sd,      # temp effect on sigma,
      sigma = exp(log_sigma)
    )
  
  # sample 50 fake datasets
  dataset <- dataset %>%
    rowwise() %>%
    mutate(
      draw = list(seq(from = 1, to = n_sims, by = 1)),
      sales = list(rnorm(n_sims, mu, sigma) + rnorm(n_sims, 0, res.sd))
    ) %>%
    unnest(cols = c("draw", "sales")) %>%
    select(month, temp, draw, sales)
  
  # calculate causal support for each simulated dataset
  results <- tibble()
  for (j in 1:n_sims) {
    # get one dataset to use
    use_data <- dataset %>%
      filter(draw == j) %>%
      select(-draw)
    
    # calc causal support
    cs <- causal_support(
      d = use_data,
      outcome = "sales", 
      predictors = list("month", "temp"), 
      target_term ="temp",
      sigma_submodel = "~temp"
    )
    
    # store results
    result <- tibble(
        use_data, # important that this is not named
        causal_support = cs,
        normative_belief = plogis(cs),
        effect_size = pearsons.r[i],
        draw = j
      ) %>%
      group_by(causal_support, normative_belief, effect_size, draw) %>%
      nest()
    results <- bind_rows(results, result)
  }
  
  # append resulting datasets to output
  datasets.month.temp <- bind_rows(datasets.month.temp, results)
}

head(datasets.month.temp)
```

In this information condition, causal support provides a benchmark of how a rational Bayesian agent would rate the probability of the following claim:

_"Temperature has an impact on sales (after accounting for the impact of month on sales)."_

Now, we'll pick 3 fake datasets per level of task difficulty (one dataset to show for each modelcheck condition), resulting in 9 datasets total for this information condition.

```{r}
for (i in 1:length(target_normative_response)) {
  # select generated datasets closest to target causal support
  selected_datasets <- datasets.month.temp %>%
    mutate(
      diff_from_target = abs(target_normative_response[i] - normative_belief)
    ) %>%
    slice_min(diff_from_target, n = 3) %>%
    select(causal_support, normative_belief, data)
  
  # add correctly specified model check to second selected dataset
  selected_datasets[2, "data"] <- modelcheck(
      "sales ~ month",
      "~temp",
      unnest(selected_datasets[2, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # add mis-specified model check to third selected dataset
  selected_datasets[3, "data"] <- modelcheck(
      "sales ~ 1",                    # ignore real confound
      "~temp",                        
      unnest(selected_datasets[3, "data"], cols = c(data))
    ) %>% 
    nest(data = everything())
  
  # create the data structure we want
  save_datasets[[paste("dataonly", target_normative_response[i], "interaction",  sep = "_")]] <- selected_datasets[1,]
  save_datasets[[paste("modelcheck", target_normative_response[i], "interaction", sep = "_")]] <- selected_datasets[2,]
  save_datasets[[paste("misspec", target_normative_response[i], "interaction", sep = "_")]] <- selected_datasets[3,] 
}
```

Save datasets. We'll push these synthetic datasets to Firebase and query them by information condition name on the fly in order to run our experiment in a custom web application.

```{r eval=FALSE}
# save datasets as json (ready to upload to firebase)
write(toJSON(save_datasets), "stimuli.json")
```


